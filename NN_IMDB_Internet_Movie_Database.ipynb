{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras from Tensorflow\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Dataset from the Internet Movie Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we define the variables train_data and test_data which are lists of reviews; each review is a list of word indices (encoding a sequence of words). Then, train_labels and test_labels are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IMDB dataset is a set of 50k highly polarized reviews from the Internet Movie Database. \n",
    "\n",
    "# They are split into 25k reviews for training and 25k for testing, each set consisting of 50% negative and 50% positive reviews. \n",
    "# num_words = 10k meams you're keeping the top 10k most frequently occurring words in the training data\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PREPARING THE DATA\n",
    "\n",
    "We can't feed lists of integers into a neural network: we have to turn these lists into tensors. In what follows, we are going to vectorize those sequences through a vectorization function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension = 10000): # dimension = 10k as the num_words\n",
    "    \"\"\"\n",
    "    This function performs a one-hot encoding of the 'sequences' list turning it into a vector of 0s and 1s. To be more concrete, this function\n",
    "    turns the sequence [3, 5] into a 10.000-dimensional vector that would be all 0s except fot indices 3 and 5, which would be 1s.\n",
    "    \"\"\"\n",
    "    results = np.zeros((len(sequences), dimension)) # creates an all-zero matrix of shape (len(sequences), dimension)\n",
    "                       \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "                       \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of both training and testing data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of labels (straightforward)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING THE NEURAL NETWORK \n",
    "\n",
    "Input data are vectors, and the labels are scalars: a type of network that performs well on such a problem is a simple stack of fully connected (Dense) layers with 'relu' activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a standard NN \n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Giving a structure to the NN: we introduce three layers - the 1st layer of 16 neurons receiving 10k inputs, the 2ns of 16 neurons and the last \n",
    "# containing 1 neuron only. \n",
    "model.add(layers.Dense(16, input_shape = (10000,), activation = 'relu'))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we should choose a loss function and an optimizer. Because we're facing a binary classification problem and the output of our network is a\n",
    "# probability, it's best to use the 'binary_crossentropy' loss (the best choice when dealing with models that output probabilities).\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate = 0.001)                                     # OPTIMIZER CHOICE\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])      # COMPILING THE MODEL CHOOSING THE LOSS/COST FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to monitor during training the accuracy of the model on data, we can create a validation set by setting apart 10k samples from the original \n",
    "# data \n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING \n",
    "\n",
    "# We train the model for 20 epochs (20 iterations over all samples in the x_train and y_train tensors), in mini-batches of 512 samples. At the same\n",
    "# time, we monitor loss and accuracy on the 10k samples that have been set apart. This can be done by passing the validation data as an argument.\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to look what happpened during training by accessing to the history's dictionary \n",
    "\n",
    "history_dict = history.history   # accessing the dictionary\n",
    "history_dict.keys()              # the dictionary has four entries: one per metric that was being monitores during training and during validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "num_epochs = 20 # as defined during training \n",
    "\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.plot(epochs, loss_values, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracy \n",
    "\n",
    "plt.clf() # used to clear the figure\n",
    "\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "The training loss decreased with every epoch, and the training accuracy increases with every epoch. That's what we expect when running optimization algorithms such as gradient-descent - the quantity we want to minimize, i.e. the cost function, should be less with every iterations.\n",
    "\n",
    "This is not the case of the validation loss and accuracy: they seem to peak at the fourth epoch. This is a clear case of overfitting: after the second epoch, we are overoptimizing on the training data, and we end up learning representations that are specific to the training data and don't generlize to data outside of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREVENT OVERFITTING \n",
    "\n",
    "In this case, to prevent overfitting, we could stop training after three epochs (this is not necessarily the optimal choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retraining a model from scratch \n",
    "model_2 = keras.Sequential()\n",
    "model_2.add(layers.Dense(16, input_shape = (10000,), activation = 'relu'))\n",
    "model_2.add(layers.Dense(16, activation = 'relu'))\n",
    "model_2.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.RMSprop(learning_rate = 0.001)                                     # OPTIMIZER CHOICE\n",
    "model_2.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['accuracy'])      # COMPILING THE MODEL CHOOSING THE LOSS/COST FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(x_train, y_train, epochs=4, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_2.evaluate(x_test, y_test)\n",
    "\n",
    "test_loss = results[0];\n",
    "test_accuracy = results[1];\n",
    "\n",
    "print(' ')\n",
    "print('*** METRICS FOR THE TESTING SET ***')\n",
    "print('The accuracy in predicting the testing set is %g %%' %(test_accuracy*100))\n",
    "print('The loss in predicting the testing set is %g %%' %(test_loss*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
